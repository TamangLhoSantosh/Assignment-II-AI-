{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVW-9VUutLW9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive # mounts the google drive for a new notebook \n",
    "print('Mounting Drive ...')\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8NfMiW7uDMO"
   },
   "outputs": [],
   "source": [
    "# load the 2 npy files created by the process_yale_images.ipynb \n",
    "from numpy import load\n",
    "import numpy as np\n",
    "path = '/content/drive/My Drive/Assignment II/'\n",
    "# load array\n",
    "print(\"Reading Data ...\")\n",
    "X = load(path + 'Copy of yaleExtB_data.npy')\n",
    "print('Data Reading Completed.')\n",
    "print('Reading Target Data ...')\n",
    "target = load(path + 'Copy of yaleExtB_target.npy')\n",
    "print(\"Target Data Reading Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmgqOq52ZHmA"
   },
   "source": [
    "# Using TensorFlow For Meta Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efWy28xLuJxp"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "reshaped = target.reshape(len(target), 1)\n",
    "Y = onehot_encoder.fit_transform(reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqtH_uj_xz1k"
   },
   "outputs": [],
   "source": [
    "# splitting training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.33, random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCs69kLHbnSR"
   },
   "outputs": [],
   "source": [
    "# calculating pca\n",
    "from sklearn.decomposition import PCA # Used to perform Principal Component Analysis\n",
    "\n",
    "# Defining the number of principal components to use for PCA.\n",
    "nof_prin_components = 200\n",
    "\n",
    "# Initializing a PCA object with the desired number of principal components and fits it to the training data.\n",
    "pca = PCA(n_components=nof_prin_components, whiten=True).fit(X_train)\n",
    "\n",
    "# Transforming the training data into the new feature space defined by the principal components.\n",
    "X_train_pca = pca.transform(X_train)\n",
    "\n",
    "# Transforming the testing data into the new feature space defined by the principal components.\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Trasforming the whole dataset into the new feature space defined bt the principal components.\n",
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJxR6gVAZTxs"
   },
   "outputs": [],
   "source": [
    "# configuring a model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(30, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer ='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_pca, y_train, batch_size=5, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUE7gmlZbNZC"
   },
   "outputs": [],
   "source": [
    "# Testing Model\n",
    "loss, accuracy = model.evaluate(X_test_pca, y_test, verbose=0)\n",
    "\n",
    "print('Test Loss : ', loss)\n",
    "print('Test accuracy : ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFr5bF_ObUij"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7l8GkLolbWbZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "actual = np.argmax(y_test, axis=1)\n",
    "predicted = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(f'Actual : {actual}')\n",
    "print(\"Predicted : \", predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mszRVyP9XGdR"
   },
   "source": [
    "## Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhxylevTXLDy"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size = 0.33, random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M051qymesvRq"
   },
   "outputs": [],
   "source": [
    "# calculating pca\n",
    "from sklearn.decomposition import PCA # Used to perform Principal Component Analysis\n",
    "\n",
    "# Defining the number of principal components to use for PCA.\n",
    "nof_prin_components = 200\n",
    "\n",
    "# Initializing a PCA object with the desired number of principal components and fits it to the training data.\n",
    "pca = PCA(n_components=nof_prin_components, whiten=True).fit(X_train)\n",
    "\n",
    "# Transforming the training data into the new feature space defined by the principal components.\n",
    "X_train_pca = pca.transform(X_train)\n",
    "\n",
    "# Transforming the testing data into the new feature space defined by the principal components.\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Trasforming the whole dataset into the new feature space defined bt the principal components.\n",
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wx3IKEKqxTQL"
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "adaboost = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search in AdaBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH0G4heLawIm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(adaboost, param_grid=parameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_para = grid_search.best_params_\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Best Model : \", best_model)\n",
    "print(\"Best Parameter\", best_para)\n",
    "print('Accuracy : ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search in AdaBooosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEHkBGsHaAfB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(adaboost, param_distributions=parameters, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_para = random_search.best_params_\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Best Model : \", best_model)\n",
    "print(\"Best Parameter\", best_para)\n",
    "print('Accuracy : ', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP1X3U0EGgFJ+zVl0Wk+7oS",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
